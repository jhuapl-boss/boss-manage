"""State machine that finds all cuboids in a given channel and enqueues their
S3 object keys in SQS.

Inputs:
{
    "config": {
      "object_store_config": {
        "id_count_table": "idCount.domain.boss",
        "page_in_lambda_function": "multiLambda-domain-boss",
        "page_out_lambda_function": "multiLambda-domain-boss",
        "cuboid_bucket": "cuboids.domain.boss",
        "s3_index_table": "s3index.domain.boss",
        "id_index_table": "idIndex.domain.boss",
        "s3_flush_queue": "https://queue.amazonaws.com/...",
        "id_index_new_chunk_threshold": 100,
        "index_deadletter_queue": "https://queue.amazonaws.com/...",
        "index_cuboids_keys_queue": "https://queue.amazonaws.com/..."
      },
      "kv_config": {
        "cache_host": "cache.domain.boss",
        "read_timeout": 86400,
        "cache_db": "0"
      },
      "state_config": {
        "cache_state_db": "0",
        "cache_state_host": "cache-state.domain.boss"
      }
    },
    "id_supervisor_step_fcn": "arn:aws:states:...",
    "id_cuboid_supervisor_step_fcn": "arn:aws:states:...",
    "index_dequeue_cuboids_step_fcn": "arn:aws:states:...",
    "id_index_step_fcn": "arn:aws:states:...",
    "batch_enqueue_cuboids_step_fcn": "arn:aws:states:...",
    "fanout_enqueue_cuboids_step_fcn": "arn:aws:states:...",
    "max_cuboid_fanout": int,           # Max number of Index.CuboidSupervisors to fanout at a time.
    "max_items": int,                   # Max items to retrieve from Dynamo at a time.
    "index_enqueue_cuboids_step_fcn": "arn:...",
    "lookup_key": "...",            # collection&exp&chan&res
    "exclusive_start_key": "",      # Should be empty string.
    "first_time": True
}
"""

version: '1.0'
timeout: 86400      # 1 day

while '$.first_time' == True or '$.exclusive_start_key' != '':
    """WhileStillTraversingLookupKeyIndex
    """
    Pass()
        """UpdateOperationFieldQuery
        Simply adds the name of the operation so it can be logged properly in 
        the deadletter queue in case of a failure when querying DynamoDB.
        """
        result: '$.operation'
        data: 
            ['QueryS3Index']

    Lambda('indexFindCuboidsLambda')
        """QueryS3IndexTable

        index_find_cuboids_lambda.py
        """
        retry [] 60 3 2.0
        catch []: '$.result'
            Lambda('indexWriteFailedLambda')
                """S3ReadFailed
                """
                retry ['KeyError'] 1 0 1.0
                retry [] 10 2 2.0
                catch []: '$.dlqresult'
                    Fail('Exception', 'Failed to write to dead letter queue')
                        """FailedSendingReadFailToDeadLetterQueue
                        """
            Fail('Exception', 'Failed to read from S3 index table')
                """FailedReadingS3Index
                """

    Pass()
        """UpdateOperationFieldStartSfn
        Simply adds the name of the operation so it can be logged properly in 
        the deadletter queue in case of a failure when starting the step 
        function.
        """
        result: '$.operation'
        data: 
            ['StartEnqueueCuboidsSfn']

    Lambda('startSfnLambda')
        """AsynchEnqueueCuboids

        start_sfn_lambda.py

        Starts Index.FanoutEnqueueCuboids.
        The arn is set by the QueryS3IndexTable task state's lambda.
        """
        retry [
            'StateMachineDoesNotExist', 
            'InvalidArn',
            'KeyError',
            'TypeError'
        ] 1 0 1.0
        retry [] 60 3 2.0
        catch []: '$.result'
            Lambda('indexWriteFailedLambda')
                """FanoutEnqueueFailed
                """
                retry ['KeyError'] 1 0 1.0
                retry [] 10 2 2.0
                catch []: '$.dlqresult'
                    Fail('Exception', 'Failed to write to dead letter queue')
                        """FailedSendingFanoutToDeadLetterQueue
                        """
            Fail('Exception', 'Failed to start fanout enqueuing step function')
                """FailedFanningOutEnqueue
                """

# Asynch invoke step function to start indexing.
Lambda('indexInvokeIndexSupervisorLambda')
    """StartIndexing
    
    invoke_index_supervisor_lambda.py
    """
    retry [
        'StateMachineDoesNotExist', 
        'InvalidArn',
        'KeyError',
        'TypeError'
    ] 1 0 1.0
    retry [] 60 3 2.0
    catch []: '$.result'
        Fail('Exception', 'Failed to start Index.Supervisor')
            """FailedStartingIndexSupervisor

            No need to send this to the deadletter queue since indexing
            has not started.  Just need to start Index.Supervisor to
            resume where things broke.
            """

Success()
    """Success
    """
